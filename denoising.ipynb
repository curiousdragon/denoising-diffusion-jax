{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, List, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import value_and_grad, jit, random\n",
    "from jax import Array\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update(\"jax_array\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(key, in_dim: int, out_dim: int) -> Tuple[Array, Array]:\n",
    "    \"\"\"\n",
    "    Initialize weight and bias terms for a layer that takes in inputs with\n",
    "    dimension in_dim and outputs with dimension out_dim.\n",
    "\n",
    "    in_dim: dimension of the input, i.e. in_features x 1\n",
    "    out_dim: dimension of the output, i.e. out_features x 1\n",
    "\n",
    "    Return the weight matrix and bias vector for the layer.\n",
    "\n",
    "    weight: out_features x in_features\n",
    "    bias: out_features x 1\n",
    "    \"\"\"\n",
    "    w_key, b_key = random.split(key)\n",
    "    bound = jnp.sqrt(1 / in_dim)\n",
    "    weight = random.uniform(w_key, (out_dim, in_dim), minval=-bound, maxval=bound)\n",
    "    bias = random.uniform(b_key, (out_dim,), minval=-bound, maxval=bound)\n",
    "    return weight, bias\n",
    "\n",
    "\n",
    "def init_model(key, sizes: List[int]) -> List[Tuple[Array, Array]]:\n",
    "    \"\"\"\n",
    "    Initialize a model with layer input and output dimentions from sizes.\n",
    "    When sizes is of length N, the resulting model will contain N-1 layers.\n",
    "\n",
    "    key: random.PRNGKey(seed), used for randomness in initialization\n",
    "    sizes: list of integers, where sizes[i] is the in_dim and sizes[i+1] is\n",
    "        the out_dim for layer i.\n",
    "\n",
    "    Return a list of tuples, each tuple containing a layer's (weight, bias).\n",
    "    \"\"\"\n",
    "    n = len(sizes)\n",
    "    key, *layer_keys = random.split(key, num=n)\n",
    "    model = []\n",
    "    for i in jnp.arange(n - 1):\n",
    "        layer = init_layer(layer_keys[i], sizes[i], sizes[i + 1])\n",
    "        model.append(layer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sigmoid(x: Array) -> Array:\n",
    "    \"\"\"\n",
    "    Compute the element-wise sigmoid of x.\n",
    "    \"\"\"\n",
    "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "\n",
    "@jit\n",
    "def predict(model: List[Tuple[Array, Array]], x: Array) -> Array:\n",
    "    \"\"\"\n",
    "    Return the prediction for x given the model.\n",
    "\n",
    "    x: input to the model to be predicted\n",
    "    \"\"\"\n",
    "    for weight, bias in model:\n",
    "        x = sigmoid(jnp.dot(weight, x) + bias)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def forward_diffuse(key, x: Array, var_schedule: Array) -> Array:\n",
    "    \"\"\"\n",
    "    Run the forward diffusion process on x for the length of the given\n",
    "    variance schedule.\n",
    "\n",
    "    x: input to be diffused\n",
    "    var_schedule: variance schedule of length t (timesteps)\n",
    "\n",
    "    Return a sample from the diffused distribution at time t.\n",
    "    \"\"\"\n",
    "    alpha = jnp.prod(1 - var_schedule)\n",
    "    mean = jnp.sqrt(alpha) * x\n",
    "    cov = (1 - alpha) * jnp.eye(len(x))\n",
    "    return random.multivariate_normal(key, mean, cov)\n",
    "\n",
    "\n",
    "def reverse_diffuse_with(mean_fn: Callable[[Array, int], Array]) -> Callable:\n",
    "    \"\"\"\n",
    "    Return a reverse_diffuse function that uses the given mean_fn\n",
    "    for determining the mean at each timestep in the reverse process.\n",
    "\n",
    "    mean_fn: function that takes in (x, t) where\n",
    "        x: current state\n",
    "        t: current timestep\n",
    "    \"\"\"\n",
    "\n",
    "    @jit\n",
    "    def _reverse_diffuse(key, x: Array, var_schedule: Array) -> Array:\n",
    "        \"\"\"\n",
    "        Run the reverse diffusion process on x for the length of the given\n",
    "        variance schedule.\n",
    "\n",
    "        x: input to be diffused\n",
    "        var_schedule: variance schedule of length t (timesteps)\n",
    "\n",
    "        Return a sample from the reversed distribution at time t.\n",
    "        \"\"\"\n",
    "        subkeys = random.split(key, num=len(var_schedule))\n",
    "        for t in jnp.flip(jnp.arange(len(var_schedule))):\n",
    "            mean = mean_fn(x, t)\n",
    "            cov = jnp.dot(var_schedule[t], jnp.eye(len(x)))\n",
    "            x = random.multivariate_normal(subkeys[t], mean, cov)\n",
    "        return x\n",
    "\n",
    "    return _reverse_diffuse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mse_loss(output: Array, target: Array) -> Array:\n",
    "    \"\"\"\n",
    "    Return the MSE (Mean Squared Error) loss between the target and output.\n",
    "    \"\"\"\n",
    "    D = jnp.prod(jnp.array(target.shape))\n",
    "    error = target - output\n",
    "    return jnp.sum((error).dot(error)) / D\n",
    "\n",
    "\n",
    "@jit\n",
    "def target_fn(x_init: Array, x: Array, var_schedule: Array) -> Tuple[Array, Array]:\n",
    "    \"\"\"\n",
    "    Given (flattened) x_init and x, compute the target mean\n",
    "    and target covariance.\n",
    "\n",
    "    x_init: initial input\n",
    "    x: sampled input after diffusing x_init for t timesteps\n",
    "    var_schedule: variance schedule of length t (timesteps)\n",
    "    \"\"\"\n",
    "    eps = 1e-9\n",
    "\n",
    "    beta_t = var_schedule[-1]\n",
    "    alpha = jnp.prod((1 - var_schedule)[:-1])\n",
    "    alpha_t = jnp.dot(alpha, 1 - beta_t)\n",
    "\n",
    "    mean_init = jnp.sqrt(alpha) * (beta_t / (1 - alpha_t)) * x_init\n",
    "    mean_t = jnp.sqrt(alpha_t) * ((1 - alpha) / (1 - alpha_t)) * x\n",
    "\n",
    "    target_mean = mean_init + mean_t\n",
    "    target_cov = ((1 - alpha) / (1 - alpha_t)) * jnp.eye(len(x)) + eps\n",
    "    return target_mean, target_cov\n",
    "\n",
    "\n",
    "def kl_divergence_with(output_mean_fn: Callable[[Array, int], Array]) -> Callable:\n",
    "    \"\"\"\n",
    "    Wrapper to take in the output mean function approximator.\n",
    "    output_mean_fn should take in the (flattened) state x and timestep t,\n",
    "    and compute the output mean.\n",
    "    \"\"\"\n",
    "\n",
    "    @jit\n",
    "    def _kl_divergence(x_init: Array, x: Array, var_schedule: Array) -> Array:\n",
    "        output_mean = output_mean_fn(x, len(var_schedule))\n",
    "        output_cov = jnp.dot(var_schedule[-1], jnp.eye(len(x)))\n",
    "        output_logvar = jnp.log(jnp.diag(output_cov))\n",
    "\n",
    "        target_mean, target_cov = target_fn(x_init, x, var_schedule)\n",
    "        target_logvar = jnp.log(jnp.diag(target_cov))\n",
    "\n",
    "        logvar = output_logvar - target_logvar + jnp.exp(target_logvar - output_logvar)\n",
    "        mean = jnp.square(target_mean - output_mean) * jnp.exp(-output_logvar)\n",
    "        return jnp.mean(0.5 * (-1.0 + logvar + mean)) / jnp.log(2.0)\n",
    "\n",
    "    return _kl_divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loss(\n",
    "    key,\n",
    "    model: List[Tuple[Array, Array]],\n",
    "    x_init: Array,\n",
    "    var_schedule: Array,\n",
    ") -> Array:\n",
    "    \"\"\"\n",
    "    Compute the KL divergence loss with running the forward diffusion process\n",
    "    for t timesteps, where t is uniformly sampled from the length of the\n",
    "    variance schedule.\n",
    "    \"\"\"\n",
    "    t_key, x_key = random.split(key, num=2)\n",
    "    t = int(random.uniform(t_key, minval=1, maxval=len(var_schedule) + 1))\n",
    "    var_schedule = var_schedule[:t]\n",
    "    x = forward_diffuse(x_key, x_init, var_schedule)\n",
    "    output_mean_fn = lambda x, t: predict(model, jnp.append(x, t))\n",
    "    return kl_divergence_with(output_mean_fn)(x_init, x, var_schedule)\n",
    "\n",
    "\n",
    "def update(\n",
    "    key,\n",
    "    model: List[Tuple[Array, Array]],\n",
    "    x: Array,\n",
    "    step_size: float,\n",
    "    var_schedule: Array,\n",
    ") -> Tuple[Array, List[Tuple[Array, Array]]]:\n",
    "    loss, grads = value_and_grad(training_loss, argnums=1)(key, model, x, var_schedule)\n",
    "\n",
    "    steps = []\n",
    "    for (weight, bias), (dweight, dbias) in zip(model, grads):\n",
    "        steps.append((weight - step_size * dweight, bias - step_size * dbias))\n",
    "    return loss, steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = random.PRNGKey(1)\n",
    "model = init_model(model_key, [10, 100, 100, 9])\n",
    "\n",
    "# hyperparameters to be changed\n",
    "step_size = 0.1\n",
    "num_epochs = 10\n",
    "var_schedule = jnp.array([0.1 * i for i in range(1, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = jnp.array([\n",
    "    jnp.eye(3).flatten()\n",
    "] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:36<00:00, 15.62s/it]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(3)\n",
    "epoch_losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    loss_values = []\n",
    "    for x_init in training_data:\n",
    "        key, subkey = random.split(key)\n",
    "        loss, model = update(subkey, model, x_init, step_size, var_schedule)\n",
    "        loss_values.append(loss)\n",
    "    epoch_losses.append(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled timesteps t: 3\n",
      "Loss results:\n",
      "mse: 0.09739715605974197, kl: 0.37557253241539\n"
     ]
    }
   ],
   "source": [
    "output_mean_fn = lambda x, t: predict(model, jnp.append(x, t))\n",
    "reverse_diffuse = reverse_diffuse_with(output_mean_fn)\n",
    "kl_divergence = kl_divergence_with(output_mean_fn)\n",
    "\n",
    "key = random.PRNGKey(6)\n",
    "t_key, x_init_key, forward_key, backward_key = random.split(key, num=4)\n",
    "\n",
    "var_schedule = jnp.array([0.1 * i for i in range(1, 10)])\n",
    "t = int(random.uniform(t_key, minval=1, maxval=len(var_schedule) + 1))\n",
    "var_schedule = var_schedule[:t]\n",
    "\n",
    "# choose the starting matrix that we want to learn\n",
    "# note: size of x_init depends on the size of the model\n",
    "# model's initial input size (in_dim) is:\n",
    "# size of FLATTENED x_init + 1 (since we append t at the end)\n",
    "x_init = jnp.eye(3)\n",
    "x_init_flat = x_init.flatten()\n",
    "\n",
    "# run the forward and learned reverse process\n",
    "x_t = forward_diffuse(forward_key, x_init_flat, var_schedule)\n",
    "x_hat = reverse_diffuse(backward_key, x_t, var_schedule)\n",
    "\n",
    "# calculate the losses\n",
    "mse = mse_loss(x_hat, x_init_flat)\n",
    "kl = kl_divergence(x_init_flat, x_t, var_schedule)\n",
    "\n",
    "print(f\"Sampled timesteps t: {t}\")\n",
    "print(f\"Loss results:\")\n",
    "print(f\"mse: {mse}, kl: {kl}\")\n",
    "\n",
    "states = [x_init, x_t.reshape(x_init.shape), x_hat.reshape(x_init.shape)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d80b7f00d844c2817c69221a0b29bf",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=900.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(states)\n",
    "_, axs = plt.subplots(nrows=1, ncols=n, figsize=(n * 3, 2))\n",
    "# plot matrices from left to right\n",
    "for i in range(n):\n",
    "    axs[i].imshow(states[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run this if there are too many plots open\n",
    "# \"Figure x\" at the top of the current figure means there are x plots open;\n",
    "# the notebook might complain once x >= 20\n",
    "\n",
    "# for _ in jnp.arange(30):\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: below this is mainly for debugging purposes... potentially remove and/or\n",
    "reuse some of the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run init_network / init_layer\n",
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key)\n",
    "network = init_model(subkey, [10, 5, 5, 10])\n",
    "for x in network:\n",
    "    print(x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run predict\n",
    "t = 3\n",
    "x = jnp.eye(3)\n",
    "xt = jnp.append(x.flatten(), t)\n",
    "# predict(network, xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage of reverse_diffuse_with \n",
    "# run forward_diffuse and reverse_diffuse\n",
    "reverse_diffuse = reverse_diffuse_with(lambda x, t: x)\n",
    "\n",
    "%timeit forward_diffuse(random.PRNGKey(0), jnp.eye(3), jnp.array([0.1, 0.1, 0.1]))\n",
    "%timeit reverse_diffuse(random.PRNGKey(0), jnp.eye(3), jnp.array([0.1, 0.1, 0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualizing forward and reverse diffusion\n",
    "n = 4\n",
    "x = jnp.eye(3)\n",
    "var_schedule = jnp.array([0.1, 0.2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward diffusion\n",
    "_, axs = plt.subplots(nrows=1, ncols=n, figsize=(9, 2))\n",
    "axs[0].imshow(x)\n",
    "for t in jnp.arange(1, n):\n",
    "    key, subkey = random.split(key)\n",
    "    state = forward_diffuse(subkey, x, int(t), var_schedule)\n",
    "    axs[t].imshow(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse diffusion\n",
    "x = state\n",
    "_, axs = plt.subplots(nrows=1, ncols=n, figsize=(9, 2))\n",
    "axs[0].imshow(x)\n",
    "for t in jnp.arange(1, n):\n",
    "    key, subkey = random.split(key)\n",
    "    axs[t].imshow(reverse_diffuse(subkey, x, int(t), var_schedule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run kl divergence\n",
    "kl_divergence = kl_divergence_with(lambda x, t: x)\n",
    "kl_divergence(jnp.zeros((3, 3)), jnp.ones((3, 3)), 3, jnp.array([0.5, 0.5, 0.5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "74da0d11463147640a4316afec5e99a6a6eb153ba7d9ca6562a169804ac9f9ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
